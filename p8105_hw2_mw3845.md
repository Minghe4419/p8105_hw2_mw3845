p8105_hw2_mw3845
================
Minghe Wang
2024-09-29

## Problem 1

###### read and clean the data

``` r
#read and clean the data
subway_df = 
  read_csv('./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv', na = "",
    col_types = cols(
    Route8 = col_character(),
    Route9 = col_character(),
    Route10 = col_character(),
    Route11 = col_character())
    ) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(
    entry = ifelse(entry=='YES', TRUE, FALSE)
  )
```

So far, we read and clean the data of NYC’s subway. The data contains
`line`, `station_name`, `station_latitude`, `station longitude`,
`route1` to `route11`, `entry`, `vending`, `entrance_type`, and `ada`.

- Cleaning steps: the variables’ names are converting into lower snake
  case; the 19 variables mentioned above are selected; and `entry` is
  converted into logical variable (`YES` to `TRUE`, `NO` to `FALSE`).
- Dimension: the dimension of resulting dataset is 1868 x 19.

###### Analyze dataset with unique stations:

``` r
uni_station_df = distinct(subway_df, line, station_name, .keep_all=TRUE) |>
  mutate(
    vending = ifelse(vending=='YES', TRUE, FALSE)
  )
```

- How many distinct stations are there?

  *There are 465 stations in the dataset.*

- How many stations are ADA compliant?

  *There are 84 stations are ADA compliant.*

- What proportion of station entrances / exits without vending allow
  entrance?

  *37.7% of station entrances / exits without vending allow entrance.
  Note: Here we are not using the unique station data because a station
  might have multiple entrances / exits that the entry w/o vending could
  happen.*

``` r
subway_df |>
  filter(vending == 'NO') |>
  pull(entry) |>
  mean()
```

    ## [1] 0.3770492

###### Reformatting the data to analyze the stations that serves A train:

``` r
# pivot dataframe longer to count A train
longer_uni_station_df = pivot_longer(
  uni_station_df,
  route1:route11,
  names_to = 'route',
  values_to = 'train'
)
# filter the longer dataframe to count ADA compliant A train
a_train_df = longer_uni_station_df |>
  filter(train == 'A')
```

There are 60 distinct stations serve A train. And 17 of the stations
that serve A train are ADA compliant.

# Problem 2

Here we read the data from `Mr Trash Wheel` sheet in the excel file. For
data cleaning: the variable names are cleaned; non-data entry `...15`
and `...16` are omitted; the empty rows are omitted; and the
`sports_ball` is rounded and converted into the integer variable. Then
we add a variable `source` to this dataset so that we can better
identify the data from it after combining this dataset with other two in
the later part. **However, `homes_powered` contains 0s and NAs which
instead could be computed by `weight_tons`, whether should we correct
it?**

``` r
mr_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', 
             sheet = 'Mr. Trash Wheel') |>
  janitor::clean_names() |>
  select(-x15, -x16) |>
  filter(!if_all(everything(), is.na)) |>
  mutate(sports_balls = as.integer(round(sports_balls)),
         source = 'mr_trash_wheel',
         year = as.double(year)) # year is converted to double variable for the combination of datasets
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
tail(mr_trash_df)
```

    ## # A tibble: 6 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1      647 May    2024 2024-05-10 00:00:00        4.7                  15
    ## 2      648 May    2024 2024-05-30 00:00:00        4.13                 15
    ## 3      649 May    2024 2024-05-30 00:00:00        3.34                 15
    ## 4      650 June   2024 2024-06-11 00:00:00        3.02                 15
    ## 5      651 June   2024 2024-06-11 00:00:00        4                    15
    ## 6       NA <NA>     NA NA                      2054.                 9769
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, source <chr>

Similarly, we read and clean the datasets of `Professor Trash Wheel` and
`Gwynnda Trash Wheel` sheets in the excel. These two datasets are
slightly different from `Mr Trash Wheel` that they does not contain
non-data entry nor the `sports_balls` variable.

``` r
prof_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', sheet = 'Professor Trash Wheel') |>
  janitor::clean_names() |>
  filter(!if_all(everything(), is.na)) |>
  mutate(source = 'professor_trash_wheel')
  
gwy_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', sheet = 'Gwynnda Trash Wheel') |>
  janitor::clean_names() |>
  filter(!if_all(everything(), is.na)) |>
  mutate(source = 'gwynnda_trash_wheel')

head(gwy_trash_df)
```

    ## # A tibble: 6 × 13
    ##   dumpster month   year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 July    2021 2021-07-03 00:00:00        0.93                 15
    ## 2        2 July    2021 2021-07-07 00:00:00        2.26                 15
    ## 3        3 July    2021 2021-07-07 00:00:00        1.62                 15
    ## 4        4 July    2021 2021-07-16 00:00:00        1.76                 15
    ## 5        5 July    2021 2021-07-30 00:00:00        1.53                 15
    ## 6        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, source <chr>

``` r
combined_df = bind_rows(mr_trash_df, prof_trash_df, gwy_trash_df)

# Check the structure
str(combined_df)
```

    ## tibble [1,036 × 15] (S3: tbl_df/tbl/data.frame)
    ##  $ dumpster          : num [1:1036] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ month             : chr [1:1036] "May" "May" "May" "May" ...
    ##  $ year              : num [1:1036] 2014 2014 2014 2014 2014 ...
    ##  $ date              : POSIXct[1:1036], format: "2014-05-16" "2014-05-16" ...
    ##  $ weight_tons       : num [1:1036] 4.31 2.74 3.45 3.1 4.06 2.71 1.91 3.7 2.52 3.76 ...
    ##  $ volume_cubic_yards: num [1:1036] 18 13 15 15 18 13 8 16 14 18 ...
    ##  $ plastic_bottles   : num [1:1036] 1450 1120 2450 2380 980 1430 910 3580 2400 1340 ...
    ##  $ polystyrene       : num [1:1036] 1820 1030 3100 2730 870 2140 1090 4310 2790 1730 ...
    ##  $ cigarette_butts   : num [1:1036] 126000 91000 105000 100000 120000 90000 56000 112000 98000 130000 ...
    ##  $ glass_bottles     : num [1:1036] 72 42 50 52 72 46 32 58 49 75 ...
    ##  $ plastic_bags      : num [1:1036] 584 496 1080 896 368 ...
    ##  $ wrappers          : num [1:1036] 1162 874 2032 1971 753 ...
    ##  $ sports_balls      : int [1:1036] 7 5 6 6 7 5 3 6 6 7 ...
    ##  $ homes_powered     : num [1:1036] 0 0 0 0 0 0 0 0 0 0 ...
    ##  $ source            : chr [1:1036] "mr_trash_wheel" "mr_trash_wheel" "mr_trash_wheel" "mr_trash_wheel" ...

``` r
# Summary statistics
summary(combined_df)
```

    ##     dumpster        month                year     
    ##  Min.   :  1.0   Length:1036        Min.   :2014  
    ##  1st Qu.: 86.0   Class :character   1st Qu.:2018  
    ##  Median :199.0   Mode  :character   Median :2020  
    ##  Mean   :245.7                      Mean   :2020  
    ##  3rd Qu.:393.0                      3rd Qu.:2022  
    ##  Max.   :651.0                      Max.   :2024  
    ##  NA's   :3                          NA's   :4     
    ##       date                        weight_tons       volume_cubic_yards
    ##  Min.   :2014-05-16 00:00:00.0   Min.   :   0.610   Min.   :   5.00   
    ##  1st Qu.:2018-04-16 00:00:00.0   1st Qu.:   2.540   1st Qu.:  15.00   
    ##  Median :2020-12-26 00:00:00.0   Median :   3.080   Median :  15.00   
    ##  Mean   :2020-05-11 15:06:58.5   Mean   :   5.981   Mean   :  29.72   
    ##  3rd Qu.:2022-11-04 18:00:00.0   3rd Qu.:   3.560   3rd Qu.:  15.00   
    ##  Max.   :2024-06-11 00:00:00.0   Max.   :2054.370   Max.   :9769.00   
    ##  NA's   :4                       NA's   :1          NA's   :1         
    ##  plastic_bottles    polystyrene     cigarette_butts    glass_bottles     
    ##  Min.   :      0   Min.   :     0   Min.   :       0   Min.   :    0.00  
    ##  1st Qu.:    980   1st Qu.:   240   1st Qu.:    2800   1st Qu.:   10.00  
    ##  Median :   1900   Median :   640   Median :    4800   Median :   18.00  
    ##  Mean   :   4341   Mean   :  2749   Mean   :   26430   Mean   :   41.47  
    ##  3rd Qu.:   2900   3rd Qu.:  2100   3rd Qu.:   12000   3rd Qu.:   28.00  
    ##  Max.   :1265185   Max.   :924061   Max.   :11775200   Max.   :13755.00  
    ##  NA's   :2         NA's   :2        NA's   :2          NA's   :266       
    ##   plastic_bags       wrappers       sports_balls     homes_powered     
    ##  Min.   :     0   Min.   :     0   Min.   :   0.00   Min.   :    0.00  
    ##  1st Qu.:   221   1st Qu.:   900   1st Qu.:   6.00   1st Qu.:   39.00  
    ##  Median :   470   Median :  1450   Median :  12.00   Median :   49.83  
    ##  Mean   :  1841   Mean   :  4398   Mean   :  27.51   Mean   :   92.80  
    ##  3rd Qu.:  1120   3rd Qu.:  2590   3rd Qu.:  20.00   3rd Qu.:   58.17  
    ##  Max.   :549684   Max.   :921529   Max.   :8836.00   Max.   :30020.00  
    ##  NA's   :2        NA's   :119      NA's   :384       NA's   :70        
    ##     source         
    ##  Length:1036       
    ##  Class :character  
    ##  Mode  :character  
    ##                    
    ##                    
    ##                    
    ## 

``` r
# View the first few rows
combined_df
```

    ## # A tibble: 1,036 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,026 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, source <chr>

Write a paragraph…

# Problem 3

###### Read, clean, and wrangle

We read 3 baker-related datasets. Since the next step is to merge 3
datasets into one cleaned dataframe, we seperate the`baker_name` of
`bakers_df` into `first_name` and `last_name` so that it can match the
`baker` variables of other two datasets. The `baker` in `bakes_df` and
`results_df` only contain the bakers’ first name.

We noticed a inconsisitency issue by checking for completeness and
correctness. Note that the inconsistency in subjects are actually a
single baker’s different form of first name. Therefore, we easily solve
this issue by `mutate`-ing the values in `bakes_df` and `results_df`.

``` r
bakers_df = read_csv('./data/gbb_datasets/bakers.csv') |>
  janitor::clean_names() |>
  separate(baker_name, into = c("first_name", "last_name"), sep = " ", extra = "merge", fill = "right") |>
  select(first_name, last_name, everything())
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df = read_csv('./data/gbb_datasets/bakes.csv') |>
  janitor::clean_names() |>
  rename(first_name = baker) |>
  mutate(first_name = if_else(first_name == '"Jo"', "Jo", first_name))
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df = read_csv('./data/gbb_datasets/results.csv', skip = 2) |>
  janitor::clean_names() |>
  rename(first_name = baker) |>
  mutate(first_name = if_else(first_name == "Joanne", "Jo", first_name))
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

###### Check completeness and correctness

While checking completeness and correctness for the first time, we found
subjects with `first_name = Joanne` in `bakes_df` and
`first_name = "Jo"` in `results_df` that doesn’t match with the
`bakers_df`. After checking the source website, we identify that this
subject is Jo Wheatley from Series 2 (She uses Jo in the show as the
short form of Joanne). After mutating her name to Jo, the inconsistency
in bakers is solved.

``` r
#check for completeness and correctness
results_df |>
  anti_join(bakers_df, by = c("first_name", "series")) |>
  distinct(first_name, series)
```

    ## # A tibble: 0 × 2
    ## # ℹ 2 variables: first_name <chr>, series <dbl>

``` r
bakes_df |>
  anti_join(bakers_df, by = c("first_name", "series")) |>
  distinct(first_name, series)
```

    ## # A tibble: 0 × 2
    ## # ℹ 2 variables: first_name <chr>, series <dbl>

###### Merge datasets

We merge the 3 dataframes and then organize variables in an order that
series and episode of corresponding baker, the bakers personal info, and
then their performence in the show.

``` r
bakers_results_df <- results_df |>
  left_join(bakers_df, by = c("first_name", "series"))

full_dt <- bakers_results_df |>
  left_join(bakes_df, by = c("first_name", "series", "episode")) |>
  select('series', 'episode', 'first_name', 'last_name', 'baker_age', 'hometown', 'baker_occupation', everything())

head(full_dt)
```

    ## # A tibble: 6 × 11
    ##   series episode first_name last_name baker_age hometown      baker_occupation  
    ##    <dbl>   <dbl> <chr>      <chr>         <dbl> <chr>         <chr>             
    ## 1      1       1 Annetha    Mills            30 Essex         Midwife           
    ## 2      1       1 David      Chambers         31 Milton Keynes Entrepreneur      
    ## 3      1       1 Edd        Kimber           24 Bradford      Debt collector fo…
    ## 4      1       1 Jasminder  Randhawa         45 Birmingham    Assistant Credit …
    ## 5      1       1 Jonathan   Shepherd         25 St Albans     Research Analyst  
    ## 6      1       1 Louise     Brimelow         44 Manchester    Police Officer    
    ## # ℹ 4 more variables: technical <dbl>, result <chr>, signature_bake <chr>,
    ## #   show_stopper <chr>
