p8105_hw2_mw3845
================
Minghe Wang
2024-09-29

## Problem 1

###### read and clean the data

``` r
#read and clean the data
subway_df = 
  read_csv('./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv', na = "",
    col_types = cols(
    Route8 = col_character(),
    Route9 = col_character(),
    Route10 = col_character(),
    Route11 = col_character())
    ) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(
    entry = ifelse(entry=='YES', TRUE, FALSE)
  )
```

So far, we read and clean the data of NYC’s subway. The data contains
`line`, `station_name`, `station_latitude`, `station longitude`,
`route1` to `route11`, `entry`, `vending`, `entrance_type`, and `ada`.

- Cleaning steps: the variables’ names are converting into lower snake
  case; the 19 variables mentioned above are selected; and `entry` is
  converted into logical variable (`YES` to `TRUE`, `NO` to `FALSE`).
- Dimension: the dimension of resulting dataset is 1868 x 19.

###### Analyze dataset with unique stations:

``` r
uni_station_df = distinct(subway_df, line, station_name, .keep_all=TRUE) |>
  mutate(
    vending = ifelse(vending=='YES', TRUE, FALSE)
  )
```

- How many distinct stations are there?

  *There are 465 stations in the dataset.*

- How many stations are ADA compliant?

  *There are 84 stations are ADA compliant.*

- What proportion of station entrances / exits without vending allow
  entrance?

  *37.704918% of station entrances / exits without vending allow
  entrance. Note: Here we are not using the unique station data because
  a station might have multiple entrances / exits that the entry w/o
  vending could happen.*

###### Reformatting the data to analyze the stations that serves A train:

``` r
# pivot dataframe longer to count A train
longer_uni_station_df = pivot_longer(
  uni_station_df,
  route1:route11,
  names_to = 'route',
  values_to = 'train'
)
# filter the longer dataframe to count ADA compliant A train
a_train_df = longer_uni_station_df |>
  filter(train == 'A')
```

There are 60 distinct stations serve A train. And 17 of the stations
that serve A train are ADA compliant.

# Problem 2

Here we read the data from `Mr Trash Wheel` sheet in the excel file. For
data cleaning: the variable names are cleaned; non-data entry `...15`
and `...16` are omitted; the empty rows and non-dumpster-specific are
omitted; and the `sports_ball` is rounded and converted into the integer
variable. Then we add a variable `source` to this dataset so that we can
better identify the data from it after combining this dataset with other
two in the later part.

``` r
mr_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', 
             sheet = 'Mr. Trash Wheel') |>
  janitor::clean_names() |>
  select(-x15, -x16) |>
  filter(!if_all(date, is.na)) |>
  mutate(sports_balls = as.integer(round(sports_balls)),
         source = 'mr_trash_wheel',
         year = as.double(year)) # year is converted to double variable for the combination of datasets
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

Similarly, we read and clean the datasets of `Professor Trash Wheel` and
`Gwynnda Trash Wheel` sheets in the excel. These two datasets are
slightly different from `Mr Trash Wheel` that they does not contain
non-data entry nor the `sports_balls` variable.

``` r
prof_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', sheet = 'Professor Trash Wheel') |>
  janitor::clean_names() |>
  filter(!if_all(date, is.na)) |>
  mutate(source = 'professor_trash_wheel')
  
gwy_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', sheet = 'Gwynnda Trash Wheel') |>
  janitor::clean_names() |>
  filter(!if_all(date, is.na)) |>
  mutate(source = 'gwynnda_trash_wheel')
```

``` r
combined_df = bind_rows(mr_trash_df, prof_trash_df, gwy_trash_df)
skimr::skim(combined_df)
```

|                                                  |             |
|:-------------------------------------------------|:------------|
| Name                                             | combined_df |
| Number of rows                                   | 1032        |
| Number of columns                                | 15          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 2           |
| numeric                                          | 12          |
| POSIXct                                          | 1           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| month         |         0 |             1 |   3 |   9 |     0 |       14 |          0 |
| source        |         0 |             1 |  14 |  21 |     0 |        3 |          0 |

**Variable type: numeric**

| skim_variable      | n_missing | complete_rate |     mean |       sd |      p0 |     p25 |     p50 |      p75 |      p100 | hist  |
|:-------------------|----------:|--------------:|---------:|---------:|--------:|--------:|--------:|---------:|----------:|:------|
| dumpster           |         0 |          1.00 |   245.85 |   187.85 |    1.00 |   86.00 |  199.00 |   393.25 |    651.00 | ▇▆▃▃▃ |
| year               |         0 |          1.00 |  2019.84 |     2.90 | 2014.00 | 2018.00 | 2020.00 |  2022.00 |   2024.00 | ▅▆▅▇▆ |
| weight_tons        |         0 |          1.00 |     3.04 |     0.79 |    0.61 |    2.54 |    3.08 |     3.55 |      5.62 | ▁▃▇▃▁ |
| volume_cubic_yards |         0 |          1.00 |    15.08 |     1.29 |    5.00 |   15.00 |   15.00 |    15.00 |     20.00 | ▁▁▁▇▁ |
| plastic_bottles    |         1 |          1.00 |  2201.31 |  1634.80 |    0.00 |  980.00 | 1900.00 |  2900.00 |   9830.00 | ▇▆▁▁▁ |
| polystyrene        |         1 |          1.00 |  1383.05 |  1813.67 |    0.00 |  230.00 |  640.00 |  2045.00 |  11528.00 | ▇▂▁▁▁ |
| cigarette_butts    |         1 |          1.00 | 13295.92 | 23690.78 |    0.00 | 2800.00 | 4800.00 | 12000.00 | 310000.00 | ▇▁▁▁▁ |
| glass_bottles      |       264 |          0.74 |    20.92 |    15.08 |    0.00 |   10.00 |   18.00 |    28.00 |    110.00 | ▇▃▁▁▁ |
| plastic_bags       |         1 |          1.00 |   927.03 |  1388.81 |    0.00 |  220.00 |  470.00 |  1115.00 |  13450.00 | ▇▁▁▁▁ |
| wrappers           |       118 |          0.89 |  2246.28 |  2638.16 |    0.00 |  900.00 | 1440.00 |  2580.00 |  20100.00 | ▇▁▁▁▁ |
| sports_balls       |       381 |          0.63 |    13.98 |     9.96 |    0.00 |    6.00 |   12.00 |    20.00 |     56.00 | ▇▆▂▁▁ |
| homes_powered      |        69 |          0.93 |    46.54 |    17.89 |    0.00 |   39.00 |   49.83 |    58.08 |     93.67 | ▂▂▇▅▁ |

**Variable type: POSIXct**

| skim_variable | n_missing | complete_rate | min        | max        | median     | n_unique |
|:--------------|----------:|--------------:|:-----------|:-----------|:-----------|---------:|
| date          |         0 |             1 | 2014-05-16 | 2024-06-11 | 2020-12-26 |      540 |

In the combined dataset, there are 1032 observations key variables
including `date` when the dumpster collected, dumpster types,
`homes_powered` computed by `weight_tons`, and the generated `source`
which can identify which datasets the observation comes from. The total
weight of trash collected by Professor Trash Wheel is 246.74. The total
cigarette butts collected by Gwynnda in June of 2022 is 18120.

**Limitation of original data:**

*-Using the available data of `weighted_tons` in `prof_trash_df`, we
found that the total weight computed in the original sheet is false,
since it didn’t include the last 2 observations while taking the sum.*

*- In skim table of combined_df, the `homes_powered` contains missing
values while the `weight_tons` doesn’t. However, in
`homes powered note`,
`they`homes_powered`can be computed using the formula of`weights_tons\`
given in the notes. If there is a specific reason that they are left
un-computed, the data entriers should indicate it in the notes.*

# Problem 3

###### Read, clean, and wrangle

We read 3 baker-related datasets. Since the next step is to merge 3
datasets into one cleaned dataframe, we seperate the`baker_name` of
`bakers_df` into `first_name` and `last_name` so that it can match the
`baker` variables of other two datasets. The `baker` in `bakes_df` and
`results_df` only contain the bakers’ first name.

We noticed a inconsisitency issue by checking for completeness and
correctness. Note that the inconsistency in subjects are actually a
single baker’s different form of first name. Therefore, we easily solve
this issue by `mutate`-ing the values in `bakes_df` and `results_df`.

``` r
#bakers.csv
bakers_df = read_csv('./data/gbb_datasets/bakers.csv') |>
  janitor::clean_names() |>
  separate(baker_name, into = c("first_name", "last_name"), sep = " ", extra = "merge", fill = "right") |>
  select(first_name, last_name, everything())
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#bakes.csv
bakes_df = read_csv('./data/gbb_datasets/bakes.csv') |>
  janitor::clean_names() |>
  rename(first_name = baker) |>
  mutate(first_name = if_else(first_name == '"Jo"', "Jo", first_name))
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#results.csv
results_df = read_csv('./data/gbb_datasets/results.csv', skip = 2) |>
  janitor::clean_names() |>
  rename(first_name = baker) |>
  mutate(first_name = if_else(first_name == "Joanne", "Jo", first_name))
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

###### Check completeness and correctness

While checking completeness and correctness for the first time, we found
subjects with `first_name = Joanne` in `bakes_df` and
`first_name = "Jo"` in `results_df` that doesn’t match with the
`bakers_df`. After checking the source website, we identify that this
subject is Jo Wheatley from Series 2 (She uses Jo in the show as the
short form of Joanne). After mutating her name to Jo, the inconsistency
in bakers is solved.

``` r
#check for completeness and correctness
results_df |>
  anti_join(bakers_df, by = c("first_name", "series")) |>
  distinct(first_name, series)
```

    ## # A tibble: 0 × 2
    ## # ℹ 2 variables: first_name <chr>, series <dbl>

``` r
bakes_df |>
  anti_join(bakers_df, by = c("first_name", "series")) |>
  distinct(first_name, series)
```

    ## # A tibble: 0 × 2
    ## # ℹ 2 variables: first_name <chr>, series <dbl>

###### Merge, organize, export full datasets

We merge the 3 dataframes and then organize variables in an order that
`series` and `episode` of corresponding baker, the bakers personal info,
and their performence for each episode.

``` r
#merge bakers_df and results_df first
bakers_results_df <- results_df |>
  left_join(bakers_df, by = c("first_name", "series"))

#merge into full data
full_dt <- bakers_results_df |>
  left_join(bakes_df, by = c("first_name", "series", "episode")) |>
  select('series', 'episode', 'first_name', 'last_name', 'baker_age', 'hometown', 'baker_occupation', everything())

head(full_dt)
```

    ## # A tibble: 6 × 11
    ##   series episode first_name last_name baker_age hometown      baker_occupation  
    ##    <dbl>   <dbl> <chr>      <chr>         <dbl> <chr>         <chr>             
    ## 1      1       1 Annetha    Mills            30 Essex         Midwife           
    ## 2      1       1 David      Chambers         31 Milton Keynes Entrepreneur      
    ## 3      1       1 Edd        Kimber           24 Bradford      Debt collector fo…
    ## 4      1       1 Jasminder  Randhawa         45 Birmingham    Assistant Credit …
    ## 5      1       1 Jonathan   Shepherd         25 St Albans     Research Analyst  
    ## 6      1       1 Louise     Brimelow         44 Manchester    Police Officer    
    ## # ℹ 4 more variables: technical <dbl>, result <chr>, signature_bake <chr>,
    ## #   show_stopper <chr>

``` r
# generate path for the full dataset
file_path = file.path('./data/gbb_datasets/', "gbb_full_data.csv")

#export full data
write_csv(full_dt, file_path)
```

###### Description of data cleaning

We begin with importing 3 baker-related csv files. Then we start to
clean the datasets one by one. For `bakers_df`, the variables names are
cleaned, which also applies to other 2 dataframe. Since the other 2
dataset contains only first name for the bakers, we then create the
`first_name` variable for the convenience to merge them later. Lastly,
we re-organize the variables by placing `first_name` and `last_name` in
the front of the dataset. For `bakes_df`, we cleaned the variable names,
change `baker` to `first_name` for the consistency. Similarly, we skip
the non-data rows(notes and empty) for `results_df` and clean it.

As checking the completeness and correctness, we identify a baker who
uses different names `Joanne` `Jo` `"Jo"` in different datasets. The
information from the website of this show confirmed our assumption so
that we decide to use `Jo` as her name among the datasets. Consequently,
this inconsistency is resolved after we re-run the completeness and
correctness check.

At last, we merge the data into one full dataset, and then organize the
variables and observations in a reasonable way (illustrated in **Merge,
organize, export full datasets**). The `full_dt` contains 11 variables
and 1136 observations. The variables include both bakers’ personal info
and their performance during each episode. As we carefully clean all 3
datasets, the name standardization, the consistency and variable
alignment can be ensured. I believe the `full_dt` is ready for more
in-depth analysis.

###### Winners and Star Bakers(Series 5 - 10)

The table we created displays Star Baker for each episode and the Winner
of each Series. From the table, we observe that the baker who are most
frequently titled with Star Baker in a series does not alway become the
Winner. This result is not too surprising for me. Live Shows always tend
to surprise the audience with dramatic plots such as underdogs becoming
the winner. If we assume the baker who is entitled with Star Baker most
frequently to be the popular baker over the series, their loss of the
series Winner might stir the audience’s discussion and then make more
tags on social media for the shows.

``` r
best_bakers_df = full_dt |>
  filter(series >= 5 & 
         series <= 10 & 
         result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, first_name, last_name, result) |>
  distinct(series, episode, result, .keep_all = TRUE)
best_bakers_df
```

    ## # A tibble: 60 × 5
    ##    series episode first_name last_name   result    
    ##     <dbl>   <dbl> <chr>      <chr>       <chr>     
    ##  1      5       1 Nancy      Birtwhistle STAR BAKER
    ##  2      5       2 Richard    Burr        STAR BAKER
    ##  3      5       3 Luis       Troyano     STAR BAKER
    ##  4      5       4 Richard    Burr        STAR BAKER
    ##  5      5       5 Kate       Henry       STAR BAKER
    ##  6      5       6 Chetna     Makan       STAR BAKER
    ##  7      5       7 Richard    Burr        STAR BAKER
    ##  8      5       8 Richard    Burr        STAR BAKER
    ##  9      5       9 Richard    Burr        STAR BAKER
    ## 10      5      10 Nancy      Birtwhistle WINNER    
    ## # ℹ 50 more rows

###### Viewership Data

The `viewers_df` shows that this show is becoming more popular from
series 1 to 7 and the viewership tends to become stable since series 8.

``` r
# read data and display first 10 rows
viewers_df = read_csv('./data/gbb_datasets/viewers.csv') |>
  janitor::clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewers_df, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

As importing the `viewers.csv`, we can compute the mean of series 1 and
5:

- average viewership of series 1: 2.77

- average viewership of series 5: 10.0393
