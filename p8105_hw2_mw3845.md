p8105_hw2_mw3845
================
Minghe Wang
2024-09-29

## Problem 1

###### read and clean the data

``` r
#read and clean the data
subway_df = 
  read_csv('./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv', na = "",
    col_types = cols(
    Route8 = col_character(),
    Route9 = col_character(),
    Route10 = col_character(),
    Route11 = col_character())
    ) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(
    entry = ifelse(entry=='YES', TRUE, FALSE)
  )
```

So far, we read and clean the data of NYC’s subway. The data contains
`line`, `station_name`, `station_latitude`, `station longitude`,
`route1` to `route11`, `entry`, `vending`, `entrance_type`, and `ada`.

- Cleaning steps: the variables’ names are converting into lower snake
  case; the 19 variables mentioned above are selected; and `entry` is
  converted into logical variable (`YES` to `TRUE`, `NO` to `FALSE`).
- Dimension: the dimension of resulting dataset is 1868 x 19.

###### Analyze dataset with unique stations:

``` r
uni_station_df = distinct(subway_df, line, station_name, .keep_all=TRUE) |>
  mutate(
    vending = ifelse(vending=='YES', TRUE, FALSE)
  )
```

- How many distinct stations are there?

  *There are 465 stations in the dataset.*

- How many stations are ADA compliant?

  *There are 84 stations are ADA compliant.*

- What proportion of station entrances / exits without vending allow
  entrance?

  *37.7% of station entrances / exits without vending allow entrance.
  Note: Here we are not using the unique station data because a station
  might have multiple entrances / exits that the entry w/o vending could
  happen.*

``` r
subway_df |>
  filter(vending == 'NO') |>
  pull(entry) |>
  mean()
```

    ## [1] 0.3770492

###### Reformatting the data to analyze the stations that serves A train:

``` r
# pivot dataframe longer to count A train
longer_uni_station_df = pivot_longer(
  uni_station_df,
  route1:route11,
  names_to = 'route',
  values_to = 'train'
)
# filter the longer dataframe to count ADA compliant A train
a_train_df = longer_uni_station_df |>
  filter(train == 'A')
```

There are 60 distinct stations serve A train. And 17 of the stations
that serve A train are ADA compliant.

# Problem 2

Here we read the data from `Mr Trash Wheel` sheet in the excel file. For
data cleaning: the variable names are cleaned; non-data entry `...15`
and `...16` are omitted; the empty rows are omitted; and the
`sports_ball` is rounded and converted into the integer variable. Then
we add a variable `source` to this dataset so that we can better
identify the data from it after combining this dataset with other two in
the later part. **However, `homes_powered` contains 0s and NAs which
instead could be computed by `weight_tons`, whether should we correct
it?**

``` r
mr_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', 
             sheet = 'Mr. Trash Wheel') |>
  janitor::clean_names() |>
  select(-x15, -x16) |>
  filter(!if_all(everything(), is.na)) |>
  mutate(sports_balls = as.integer(round(sports_balls)),
         source = 'mr_trash_wheel',
         year = as.double(year)) # year is converted to double variable for the combination of datasets
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
tail(mr_trash_df)
```

    ## # A tibble: 6 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1      647 May    2024 2024-05-10 00:00:00        4.7                  15
    ## 2      648 May    2024 2024-05-30 00:00:00        4.13                 15
    ## 3      649 May    2024 2024-05-30 00:00:00        3.34                 15
    ## 4      650 June   2024 2024-06-11 00:00:00        3.02                 15
    ## 5      651 June   2024 2024-06-11 00:00:00        4                    15
    ## 6       NA <NA>     NA NA                      2054.                 9769
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, source <chr>

Similarly, we read and clean the datasets of `Professor Trash Wheel` and
`Gwynnda Trash Wheel` sheets in the excel. These two datasets are
slightly different from `Mr Trash Wheel` that they does not contain
non-data entry nor the `sports_balls` variable.

``` r
prof_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', sheet = 'Professor Trash Wheel') |>
  janitor::clean_names() |>
  filter(!if_all(everything(), is.na)) |>
  mutate(source = 'professor_trash_wheel')
  
gwy_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', sheet = 'Gwynnda Trash Wheel') |>
  janitor::clean_names() |>
  filter(!if_all(everything(), is.na)) |>
  mutate(source = 'gwynnda_trash_wheel')

head(gwy_trash_df)
```

    ## # A tibble: 6 × 13
    ##   dumpster month   year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 July    2021 2021-07-03 00:00:00        0.93                 15
    ## 2        2 July    2021 2021-07-07 00:00:00        2.26                 15
    ## 3        3 July    2021 2021-07-07 00:00:00        1.62                 15
    ## 4        4 July    2021 2021-07-16 00:00:00        1.76                 15
    ## 5        5 July    2021 2021-07-30 00:00:00        1.53                 15
    ## 6        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, source <chr>

``` r
combined_df = bind_rows(mr_trash_df, prof_trash_df, gwy_trash_df)

# Check the structure
str(combined_df)
```

    ## tibble [1,036 × 15] (S3: tbl_df/tbl/data.frame)
    ##  $ dumpster          : num [1:1036] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ month             : chr [1:1036] "May" "May" "May" "May" ...
    ##  $ year              : num [1:1036] 2014 2014 2014 2014 2014 ...
    ##  $ date              : POSIXct[1:1036], format: "2014-05-16" "2014-05-16" ...
    ##  $ weight_tons       : num [1:1036] 4.31 2.74 3.45 3.1 4.06 2.71 1.91 3.7 2.52 3.76 ...
    ##  $ volume_cubic_yards: num [1:1036] 18 13 15 15 18 13 8 16 14 18 ...
    ##  $ plastic_bottles   : num [1:1036] 1450 1120 2450 2380 980 1430 910 3580 2400 1340 ...
    ##  $ polystyrene       : num [1:1036] 1820 1030 3100 2730 870 2140 1090 4310 2790 1730 ...
    ##  $ cigarette_butts   : num [1:1036] 126000 91000 105000 100000 120000 90000 56000 112000 98000 130000 ...
    ##  $ glass_bottles     : num [1:1036] 72 42 50 52 72 46 32 58 49 75 ...
    ##  $ plastic_bags      : num [1:1036] 584 496 1080 896 368 ...
    ##  $ wrappers          : num [1:1036] 1162 874 2032 1971 753 ...
    ##  $ sports_balls      : int [1:1036] 7 5 6 6 7 5 3 6 6 7 ...
    ##  $ homes_powered     : num [1:1036] 0 0 0 0 0 0 0 0 0 0 ...
    ##  $ source            : chr [1:1036] "mr_trash_wheel" "mr_trash_wheel" "mr_trash_wheel" "mr_trash_wheel" ...

``` r
# Summary statistics
summary(combined_df)
```

    ##     dumpster        month                year     
    ##  Min.   :  1.0   Length:1036        Min.   :2014  
    ##  1st Qu.: 86.0   Class :character   1st Qu.:2018  
    ##  Median :199.0   Mode  :character   Median :2020  
    ##  Mean   :245.7                      Mean   :2020  
    ##  3rd Qu.:393.0                      3rd Qu.:2022  
    ##  Max.   :651.0                      Max.   :2024  
    ##  NA's   :3                          NA's   :4     
    ##       date                        weight_tons       volume_cubic_yards
    ##  Min.   :2014-05-16 00:00:00.0   Min.   :   0.610   Min.   :   5.00   
    ##  1st Qu.:2018-04-16 00:00:00.0   1st Qu.:   2.540   1st Qu.:  15.00   
    ##  Median :2020-12-26 00:00:00.0   Median :   3.080   Median :  15.00   
    ##  Mean   :2020-05-11 15:06:58.5   Mean   :   5.981   Mean   :  29.72   
    ##  3rd Qu.:2022-11-04 18:00:00.0   3rd Qu.:   3.560   3rd Qu.:  15.00   
    ##  Max.   :2024-06-11 00:00:00.0   Max.   :2054.370   Max.   :9769.00   
    ##  NA's   :4                       NA's   :1          NA's   :1         
    ##  plastic_bottles    polystyrene     cigarette_butts    glass_bottles     
    ##  Min.   :      0   Min.   :     0   Min.   :       0   Min.   :    0.00  
    ##  1st Qu.:    980   1st Qu.:   240   1st Qu.:    2800   1st Qu.:   10.00  
    ##  Median :   1900   Median :   640   Median :    4800   Median :   18.00  
    ##  Mean   :   4341   Mean   :  2749   Mean   :   26430   Mean   :   41.47  
    ##  3rd Qu.:   2900   3rd Qu.:  2100   3rd Qu.:   12000   3rd Qu.:   28.00  
    ##  Max.   :1265185   Max.   :924061   Max.   :11775200   Max.   :13755.00  
    ##  NA's   :2         NA's   :2        NA's   :2          NA's   :266       
    ##   plastic_bags       wrappers       sports_balls     homes_powered     
    ##  Min.   :     0   Min.   :     0   Min.   :   0.00   Min.   :    0.00  
    ##  1st Qu.:   221   1st Qu.:   900   1st Qu.:   6.00   1st Qu.:   39.00  
    ##  Median :   470   Median :  1450   Median :  12.00   Median :   49.83  
    ##  Mean   :  1841   Mean   :  4398   Mean   :  27.51   Mean   :   92.80  
    ##  3rd Qu.:  1120   3rd Qu.:  2590   3rd Qu.:  20.00   3rd Qu.:   58.17  
    ##  Max.   :549684   Max.   :921529   Max.   :8836.00   Max.   :30020.00  
    ##  NA's   :2        NA's   :119      NA's   :384       NA's   :70        
    ##     source         
    ##  Length:1036       
    ##  Class :character  
    ##  Mode  :character  
    ##                    
    ##                    
    ##                    
    ## 

``` r
# View the first few rows
combined_df
```

    ## # A tibble: 1,036 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,026 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, source <chr>

Write a paragraph…

# Problem 3

###### Read, clean, and wrangle

We read 3 baker-related datasets. Since the next step is to merge 3
datasets into one cleaned dataframe, we seperate the`baker_name` of
`bakers_df` into `first_name` and `last_name` so that it can match the
`baker` variables of other two datasets. The `baker` in `bakes_df` and
`results_df` only contain the bakers’ first name.

We noticed a inconsisitency issue by checking for completeness and
correctness. Note that the inconsistency in subjects are actually a
single baker’s different form of first name. Therefore, we easily solve
this issue by `mutate`-ing the values in `bakes_df` and `results_df`.

``` r
#bakers.csv
bakers_df = read_csv('./data/gbb_datasets/bakers.csv') |>
  janitor::clean_names() |>
  separate(baker_name, into = c("first_name", "last_name"), sep = " ", extra = "merge", fill = "right") |>
  select(first_name, last_name, everything())
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#bakes.csv
bakes_df = read_csv('./data/gbb_datasets/bakes.csv') |>
  janitor::clean_names() |>
  rename(first_name = baker) |>
  mutate(first_name = if_else(first_name == '"Jo"', "Jo", first_name))
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#results.csv
results_df = read_csv('./data/gbb_datasets/results.csv', skip = 2) |>
  janitor::clean_names() |>
  rename(first_name = baker) |>
  mutate(first_name = if_else(first_name == "Joanne", "Jo", first_name))
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

###### Check completeness and correctness

While checking completeness and correctness for the first time, we found
subjects with `first_name = Joanne` in `bakes_df` and
`first_name = "Jo"` in `results_df` that doesn’t match with the
`bakers_df`. After checking the source website, we identify that this
subject is Jo Wheatley from Series 2 (She uses Jo in the show as the
short form of Joanne). After mutating her name to Jo, the inconsistency
in bakers is solved.

``` r
#check for completeness and correctness
results_df |>
  anti_join(bakers_df, by = c("first_name", "series")) |>
  distinct(first_name, series)
```

    ## # A tibble: 0 × 2
    ## # ℹ 2 variables: first_name <chr>, series <dbl>

``` r
bakes_df |>
  anti_join(bakers_df, by = c("first_name", "series")) |>
  distinct(first_name, series)
```

    ## # A tibble: 0 × 2
    ## # ℹ 2 variables: first_name <chr>, series <dbl>

###### Merge, organize, export full datasets

We merge the 3 dataframes and then organize variables in an order that
`series` and `episode` of corresponding baker, the bakers personal info,
and their performence for each episode.

``` r
#merge bakers_df and results_df first
bakers_results_df <- results_df |>
  left_join(bakers_df, by = c("first_name", "series"))

#merge into full data
full_dt <- bakers_results_df |>
  left_join(bakes_df, by = c("first_name", "series", "episode")) |>
  select('series', 'episode', 'first_name', 'last_name', 'baker_age', 'hometown', 'baker_occupation', everything())

head(full_dt)
```

    ## # A tibble: 6 × 11
    ##   series episode first_name last_name baker_age hometown      baker_occupation  
    ##    <dbl>   <dbl> <chr>      <chr>         <dbl> <chr>         <chr>             
    ## 1      1       1 Annetha    Mills            30 Essex         Midwife           
    ## 2      1       1 David      Chambers         31 Milton Keynes Entrepreneur      
    ## 3      1       1 Edd        Kimber           24 Bradford      Debt collector fo…
    ## 4      1       1 Jasminder  Randhawa         45 Birmingham    Assistant Credit …
    ## 5      1       1 Jonathan   Shepherd         25 St Albans     Research Analyst  
    ## 6      1       1 Louise     Brimelow         44 Manchester    Police Officer    
    ## # ℹ 4 more variables: technical <dbl>, result <chr>, signature_bake <chr>,
    ## #   show_stopper <chr>

``` r
# generate path for the full dataset
file_path = file.path('./data/gbb_datasets/', "gbb_full_data.csv")

#export full data
write_csv(full_dt, file_path)
```

###### Description of data cleaning

We begin with importing 3 baker-related csv files. Then we start to
clean the datasets one by one. For `bakers_df`, the variables names are
cleaned, which also applies to other 2 dataframe. Since the other 2
dataset contains only first name for the bakers, we then create the
`first_name` variable for the convenience to merge them later. Lastly,
we re-organize the variables by placing `first_name` and `last_name` in
the front of the dataset. For `bakes_df`, we cleaned the variable names,
change `baker` to `first_name` for the consistency. Similarly, we skip
the non-data rows(notes and empty) for `results_df` and clean it.

As checking the completeness and correctness, we identify a baker who
uses different names `Joanne` ``` Jo``"Jo" ``` in different datasets.
The information from the website of this show confirmed our assumption
so that we decide to use `Jo` as her name among the datasets.
Consequently, this inconsistency is resolved after we re-run the
completeness and correctness check.

At last, we merge the data into one full dataset, and then organize the
variables and observations in a reasonable way (illustrated in **Merge,
organize, export full datasets**). The `full_dt` contains 11 variables
and 1136 observations. The variables include both bakers’ personal info
and their performance during each episode. As we carefully clean all 3
datasets, the name standardization, the consistency and variable
alignment can be ensured. I believe the `full_dt` is ready for more
in-depth analysis.

###### Winners and Star Bakers(Series 5 - 10)

The table we created displays Star Baker for each episode and the Winner
of each Series. From the table, we observe that the baker who are most
frequently titled with Star Baker in a series does not alway become the
Winner. This result is not too surprising for me. Live Shows always tend
to surprise the audience with dramatic plots such as underdogs becoming
the winner. If we assume the baker who is entitled with Star Baker most
frequently to be the popular baker over the series, their loss of the
series Winner might stir the audience’s discussion and then make more
tags on social media for the shows.

``` r
best_bakers_df = full_dt |>
  filter(series >= 5 & 
         series <= 10 & 
         result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, first_name, last_name, result) |>
  distinct(series, episode, result, .keep_all = TRUE)
best_bakers_df
```

    ## # A tibble: 60 × 5
    ##    series episode first_name last_name   result    
    ##     <dbl>   <dbl> <chr>      <chr>       <chr>     
    ##  1      5       1 Nancy      Birtwhistle STAR BAKER
    ##  2      5       2 Richard    Burr        STAR BAKER
    ##  3      5       3 Luis       Troyano     STAR BAKER
    ##  4      5       4 Richard    Burr        STAR BAKER
    ##  5      5       5 Kate       Henry       STAR BAKER
    ##  6      5       6 Chetna     Makan       STAR BAKER
    ##  7      5       7 Richard    Burr        STAR BAKER
    ##  8      5       8 Richard    Burr        STAR BAKER
    ##  9      5       9 Richard    Burr        STAR BAKER
    ## 10      5      10 Nancy      Birtwhistle WINNER    
    ## # ℹ 50 more rows

###### Viewership Data

The `viewers_df` shows that this show is becoming more popular from
series 1 to 7 and the viewership tends to become stable since series 8.

``` r
# read data and display first 10 rows
viewers_df = read_csv('./data/gbb_datasets/viewers.csv') |>
  janitor::clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewers_df, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

As importing the `viewers.csv`, we can compute the mean of series 1 and
5:

- average viewership of series 1: 2.77

- average viewership of series 5: 10.0393
