---
title: "p8105_hw2_mw3845"
author: "Minghe Wang"
date: "2024-09-29"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(readxl)
```

## Problem 1

###### read and clean the data

```{r problem1_read_and_clean_data}
#read and clean the data
subway_df = 
  read_csv('./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv', na = "",
    col_types = cols(
    Route8 = col_character(),
    Route9 = col_character(),
    Route10 = col_character(),
    Route11 = col_character())
    ) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(
    entry = ifelse(entry=='YES', TRUE, FALSE)
  )
```

So far, we read and clean the data of NYC's subway. The data contains ` line`, `station_name`, `station_latitude`, `station longitude`, `route1` to `route11`, `entry`, `vending`, `entrance_type`, and `ada`. 

- Cleaning steps: the variables' names are converting into lower snake case; the 19 variables mentioned above are selected; and `entry` is converted into logical variable (`YES` to `TRUE`, `NO` to `FALSE`).
- Dimension: the dimension of resulting dataset is `r nrow(subway_df)` x `r ncol(subway_df)`.

###### Analyze dataset with unique stations:

```{r unique station dataframe}
uni_station_df = distinct(subway_df, line, station_name, .keep_all=TRUE) |>
  mutate(
    vending = ifelse(vending=='YES', TRUE, FALSE)
  )
```
- How many distinct stations are there?
  
  _There are `r nrow(uni_station_df)` stations in the dataset._

- How many stations are ADA compliant?

  _There are `r sum(pull(uni_station_df, ada))` stations are ADA compliant._
  
- What proportion of station entrances / exits without vending allow entrance?

  _37.7% of station entrances / exits without vending allow entrance.  Note: Here we are not using the unique station data   because a station might have multiple entrances / exits that the entry w/o vending could happen._

```{r entry w/o vending}
subway_df |>
  filter(vending == 'NO') |>
  pull(entry) |>
  mean()
```

  
###### Reformatting the data to analyze the stations that serves A train:
  
```{r reformatting uni_station_df}
# pivot dataframe longer to count A train
longer_uni_station_df = pivot_longer(
  uni_station_df,
  route1:route11,
  names_to = 'route',
  values_to = 'train'
)
# filter the longer dataframe to count ADA compliant A train
a_train_df = longer_uni_station_df |>
  filter(train == 'A')
```

There are `r sum(pull(longer_uni_station_df, train) == 'A', na.rm = TRUE)` distinct stations serve A train. And `r sum(pull(a_train_df, ada))` of the stations that serve A train are ADA compliant.

# Problem 2

Here we read the data from `Mr Trash Wheel` sheet in the excel file. For data cleaning: the variable names are cleaned; non-data entry `...15` and `...16` are omitted; the empty rows are omitted; and the `sports_ball` is rounded and converted into the integer variable. Then we add a variable `source` to this dataset so that we can better identify the data from it after combining this dataset with other two in the later part. **However, `homes_powered` contains 0s and NAs which instead could be computed by `weight_tons`, whether should we correct it?**
```{r p2_read_and_clean_MrTW}
mr_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', 
             sheet = 'Mr. Trash Wheel') |>
  janitor::clean_names() |>
  select(-x15, -x16) |>
  filter(!if_all(everything(), is.na)) |>
  mutate(sports_balls = as.integer(round(sports_balls)),
         source = 'mr_trash_wheel',
         year = as.double(year)) # year is converted to double variable for the combination of datasets

tail(mr_trash_df)
```

Similarly, we read and clean the datasets of `Professor Trash Wheel` and `Gwynnda Trash Wheel` sheets in the excel. These two datasets are slightly different from `Mr Trash Wheel` that they does not contain non-data entry nor the `sports_balls` variable.

```{r p2_read_and_clean_other_2_dataset}
prof_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', sheet = 'Professor Trash Wheel') |>
  janitor::clean_names() |>
  filter(!if_all(everything(), is.na)) |>
  mutate(source = 'professor_trash_wheel')
  
gwy_trash_df = 
  read_excel('./data/202409 Trash Wheel Collection Data.xlsx', sheet = 'Gwynnda Trash Wheel') |>
  janitor::clean_names() |>
  filter(!if_all(everything(), is.na)) |>
  mutate(source = 'gwynnda_trash_wheel')

head(gwy_trash_df)
```

```{r combine 3 datasets}
combined_df = bind_rows(mr_trash_df, prof_trash_df, gwy_trash_df)

# Check the structure
str(combined_df)

# Summary statistics
summary(combined_df)

# View the first few rows
combined_df

```

Write a paragraph...

# Problem 3

###### Read, clean, and wrangle

We read 3 baker-related datasets. Since the next step is to merge 3 datasets into one cleaned dataframe, we seperate the`baker_name` of `bakers_df` into `first_name` and `last_name` so that it can match the `baker` variables of other two datasets. The `baker` in `bakes_df` and `results_df` only contain the bakers' first name.

We noticed a inconsisitency issue by checking for completeness and correctness. Note that the inconsistency in subjects are actually a single baker's different form of first name. Therefore, we easily solve this issue by `mutate`-ing the values in `bakes_df` and `results_df`.
```{r problem3_read_baker_data}
#bakers.csv
bakers_df = read_csv('./data/gbb_datasets/bakers.csv') |>
  janitor::clean_names() |>
  separate(baker_name, into = c("first_name", "last_name"), sep = " ", extra = "merge", fill = "right") |>
  select(first_name, last_name, everything())

#bakes.csv
bakes_df = read_csv('./data/gbb_datasets/bakes.csv') |>
  janitor::clean_names() |>
  rename(first_name = baker) |>
  mutate(first_name = if_else(first_name == '"Jo"', "Jo", first_name))

#results.csv
results_df = read_csv('./data/gbb_datasets/results.csv', skip = 2) |>
  janitor::clean_names() |>
  rename(first_name = baker) |>
  mutate(first_name = if_else(first_name == "Joanne", "Jo", first_name))
```
###### Check completeness and correctness

While checking completeness and correctness for the first time, we found subjects with `first_name = Joanne` in `bakes_df` and `first_name = "Jo"` in `results_df` that doesn't match with the `bakers_df`. After checking the source website, we identify that this subject is Jo Wheatley from Series 2 (She uses Jo in the show as the short form of Joanne). After mutating her name to Jo, the inconsistency in bakers is solved.
```{r check_completeness}
#check for completeness and correctness
results_df |>
  anti_join(bakers_df, by = c("first_name", "series")) |>
  distinct(first_name, series)

bakes_df |>
  anti_join(bakers_df, by = c("first_name", "series")) |>
  distinct(first_name, series)
```
###### Merge, organize, export full datasets
We merge the 3 dataframes and then organize variables in an order that `series` and `episode` of corresponding baker, the bakers personal info, and their performence for each episode.
```{r merge_data}
#merge bakers_df and results_df first
bakers_results_df <- results_df |>
  left_join(bakers_df, by = c("first_name", "series"))

#merge into full data
full_dt <- bakers_results_df |>
  left_join(bakes_df, by = c("first_name", "series", "episode")) |>
  select('series', 'episode', 'first_name', 'last_name', 'baker_age', 'hometown', 'baker_occupation', everything())

head(full_dt)
```

```{r export_data}
# generate path for the full dataset
file_path = file.path('./data/gbb_datasets/', "gbb_full_data.csv")

#export full data
write_csv(full_dt, file_path)
```

###### Description of data cleaning

We begin with importing 3 baker-related csv files. Then we start to clean the datasets one by one. For `bakers_df`, the variables names are cleaned, which also applies to other 2 dataframe. Since the other 2 dataset contains only first name for the bakers, we then create the `first_name` variable for the convenience to merge them later. Lastly, we re-organize the variables by placing `first_name` and `last_name` in the front of the dataset. For `bakes_df`, we cleaned the variable names, change `baker` to `first_name` for the consistency. Similarly, we skip the non-data rows(notes and empty) for `results_df` and clean it.

As checking the completeness and correctness, we identify a baker who uses different names `Joanne` `Jo``"Jo"` in different datasets. The information from the website of this show confirmed our assumption so that we decide to use `Jo` as her name among the datasets. Consequently, this inconsistency is resolved after we re-run the completeness and correctness check.

At last, we merge the data into one full dataset, and then organize the variables and observations in a reasonable way (illustrated in **Merge, organize, export full datasets**). The `full_dt` contains 11 variables and 1136 observations. The variables include both bakers' personal info and their performance during each episode. As we carefully clean all 3 datasets, the name standardization, the consistency and variable alignment can be ensured. I believe the `full_dt` is ready for more in-depth analysis.

###### Winners and Star Bakers(Series 5 - 10)

The table we created displays Star Baker for each episode and the Winner of each Series. From the table, we observe that the baker who are most frequently titled with Star Baker in a series does not alway become the Winner. This result is not too surprising for me. Live Shows always tend to surprise the audience with dramatic plots such as underdogs becoming the winner. If we assume the baker who is entitled with Star Baker most frequently to be the popular baker over the series, their loss of the series Winner might stir the audience's discussion and then make more tags on social media for the shows.

```{r find_winners_star_bakers}
best_bakers_df = full_dt |>
  filter(series >= 5 & 
         series <= 10 & 
         result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, first_name, last_name, result) |>
  distinct(series, episode, result, .keep_all = TRUE)
best_bakers_df
```

